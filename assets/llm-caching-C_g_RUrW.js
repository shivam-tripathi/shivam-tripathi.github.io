const e='# Notes on LLM Caching (Nov 2025)\n\nThese are working notes meant as a reference for engineers who want to understand how *caching* works across the major LLM providers as of late 2025. The ecosystem has converged on a shared idea – **don’t reprocess giant static prefixes** – but each provider implements this differently, with different guarantees, costs, and pitfalls.\n\nThis document captures the practical mechanisms, how they differ, and what actually matters when building systems.\n\n\n## 1. [OpenAI](https://platform.openai.com/docs/guides/prompt-caching) - *Prefix-KV Caching (Automatic)*\n\n\n### **Mechanism**\n\n* Enabled automatically for all models GPT-4o → GPT-5.1.\n* Works only on **exact token-prefix matches**.\n* Requires prompts to be ≥ **1024 tokens**.\n* Cache hit granularity: **128-token increments**.\n* Routing is determined by hashing the **first ~256 tokens** with an optional `prompt_cache_key`.\n* Important thing to note here is that routing key (the first ~256 tokens) is distinct from the prefix key used for caching.\n\n### **Key Idea**\n\nCaching only works if:\n\n1. You land on the **same machine** (routing stability), and\n2. The **first 1024+ tokens** of the prompt are identical.\n\n### **Why `prompt_cache_key` matters**\n\nIt stabilizes routing even if the early tokens differ (timestamps, IDs, whitespace, etc.). Without it, the request may land on a different server even if most of the prefix is identical.\n\n### **Costs**\n\nCaching is **free**:\n\n* No charges for “cache writes”\n* No charges for “cache reads”\n  You simply avoid paying input token fees for the cached prefix.\n\n### **Best Uses**\n\n* Large static system prompts\n* Tool schemas\n* Repetitive workflows with stable prefixes\n* High-volume inference where routing stability matters\n\n\n## 2. [Anthropic](https://platform.claude.com/docs/en/build-with-claude/prompt-caching) - *Block-Level Caching (Explicit)*\n\n[Link](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)\n\n### **Mechanism**\n\n* Developer explicitly marks blocks with `cache_control`.\n* Claude caches everything **above** that block: tools → system → messages.\n* Cache is hierarchical; modification at an earlier layer invalidates all later layers.\n\n### **TTL**\n\n* Default: **5 minutes**\n* Optional: **1 hour** (paid, higher cost)\n* Up to 4 cache breakpoints allowed.\n\n### **Granularity**\n\nCache checks across **content blocks**, not tokens (20-block lookback window).\n\n### **Costs**\n\n* **Cache read tokens**: ~10% of normal token cost\n* **5-minute write tokens**: ~1.25× input cost\n* **1-hour write tokens**: ~2× input cost\n* Only pay for written or read tokens; breakpoints themselves are free.\n\n### **Best Uses**\n\n* Very long documents (books, multi-pdf context)\n* Complex multi-turn conversations\n* Agents and tool use with evolving state\n* Cases where multiple sections change independently\n\n---\n\n## 3. [Google Gemini](https://ai.google.dev/gemini-api/docs/caching?lang=python) - *Explicit Object Caching + Implicit Prefix Caching*\n\n### **Two Caching Models**\n\n#### **(A) Implicit Caching (Auto)**\n\n* Works like OpenAI’s but with no control.\n* Triggers on long identical prefixes.\n* TTLs not guaranteed; hit rates unpredictable.\n\n#### **(B) Explicit Caching (Recommended)**\n\nYou create a **cache object**:\n\n```python\ncache = client.caches.create(\n    model="gemini-2.0-flash-001",\n    config=CreateCachedContentConfig(\n        system_instruction="long instructions",\n        contents=[pdf_or_video],\n        ttl="300s"\n    )\n)\n```\n\nThen reuse it via:\n\n```\ncached_content=cache.name\n```\n\n### **Characteristics**\n\n* Cache is a **standalone artifact**, not tied to a specific prompt structure.\n* Stores PDFs, videos, images, system prompts, text.\n* TTL defaults to **1 hour**, can be extended.\n* Identical to recalling a giant prefix without sending it again.\n\n### **Costs**\n\n* Cheaper read tokens\n* Write cost scales with token size and TTL duration\n* Output tokens charged normally\n* No rate-limit advantages\n\n### **Best Uses**\n\n* Video / multimodal analysis\n* Large PDF Q&A\n* Any workflow needing reusable context across hours\n* Periodically re-analyzed documents or codebases\n\n---\n\n## 4. Comparing the Three Approaches\n\n| Provider      | Type                   | Control                     | TTL                                | Minimum Length   | Cost Model            |\n| ------------- | ---------------------- | --------------------------- | ---------------------------------- | ---------------- | --------------------- |\n| **OpenAI**    | Implicit prefix KV     | Medium (`prompt_cache_key`) | 5–10 min (in-memory), 24h optional | 1024 tokens      | Free                  |\n| **Anthropic** | Explicit block caching | High                        | 5m / 1h                            | 1024–4096 tokens | Writes + reads billed |\n| **Gemini**    | Explicit cache objects | Very high                   | Any TTL (default 1h)               | 1024–4096 tokens | Writes + reads billed |\n\n---\n\n## 5. Best Practices (Across All Providers)\n\n### **1. Put static content at the beginning**\n\nEvery caching system relies on prefix-like structures.\nArrange prompts as:\n\n```\n[tools]\n[long system instructions]\n[examples]\n[user question comes last]\n```\n\n### **2. Avoid noisy early tokens**\n\nPrefixes break when:\n\n* Timestamps change\n* Randomized JSON key ordering changes\n* Invisible Unicode differences\n* Logging metadata leaks into the prompt\n\nStabilize early tokens to maximize cache hits.\n\n### **3. Reuse the same prompt “shape”**\n\nEven small structural differences (extra newline, tool order change, reordered arrays) break caches.\n\n### **4. For OpenAI: use `prompt_cache_key` when prefixes drift**\n\nThis forces stable routing and drastically increases cache hit rates.\n\n### **5. For Anthropic: use multiple breakpoints**\n\nUse:\n\n* One breakpoint after tools\n* One after system\n* One before examples\n* One before dynamic user data\n\nThis isolates changes and prevents invalidating the entire cache.\n\n### **6. For Gemini: treat cache objects like artifacts**\n\nCache them once, track metadata, extend TTL, and reuse systematically.\n\n### **7. Monitor cache metrics**\n\nHit rates, write volume, and cross-request drift patterns reveal where caching is failing silently.\n\n---\n\n## 6. When to Choose Which Provider’s Strategy\n\n### Use **OpenAI** when:\n\n* You want free, automatic caching\n* Your workload is high-volume, low-latency\n* You can standardize prefixes\n* Tool schemas and system messages are large but stable\n\n### Use **Anthropic** when:\n\n* You need explicit control\n* Cache segments change at different frequencies\n* You need consistent caching in multi-turn workflows\n* You want deterministic behavior\n\n### Use **Gemini** when:\n\n* You have huge multimodal data to reuse\n* You need long TTL stability\n* You want cache entries to feel like permanent objects\n* You don’t want your cache tied to prompt structure\n\n---\n\n## 7. Closing Thoughts\n\nCaching has quietly become one of the most important levers for:\n\n* reducing inference cost\n* reducing latency\n* improving throughput\n* enabling long-context applications\n\nBut each provider has invented its own mechanism.\nUnderstanding these differences-and designing prompts to cooperate with them-is now a core skill for platform engineers building on top of modern LLMs.\n\nUse this as a reference point as the ecosystem continues to converge.\n';export{e as default};

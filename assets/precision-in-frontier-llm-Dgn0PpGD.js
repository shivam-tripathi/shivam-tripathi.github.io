const e='# Notes on Precision in Frontier Language Models\n\n\nModern frontier language models are not trained or served in a single numerical precision. They are *precision systems*: layered, conditional, hardware-aware constructions where different parts of the model intentionally live at different numerical fidelities.\n\nThis document captures working notes on how that precision is allocated - and why - across weights, activations, and the hardware landscape of early 2026. These notes attempt to extract the underlying principles that hold across OpenAI (GPT-5.2), Google DeepMind (Gemini 3.0), and Anthropic (Sonnet 4.5) systems.\n\n---\n\n## 1. Weights\n\n**Observation.** Weights are the most aggressively :note[quantized]{content="**Intuition:** This is the art of reducing resolution.\\\n\\\n**Analogy:** Imagine describing a color. Instead of using a complex code like \'Midnight Navy Blue,\' you just call it \'Dark Blue.\' You save \'memory\' in your brain, and while you lose some nuance, the person you’re talking to still gets the general idea."} component in modern LLMs.\n\n**Why this works.** During inference, :note[weights are static]{content="**Intuition:** Think of weights as the fixed settings on a massive mixing board. They represent the strength of connections between neurons.\\\n\\\n**The \'Static\' Principle:** Because they don\'t change while the model is answering a prompt (inference), any error we introduce by \'rounding\' the numbers is fixed. It doesn\'t \'grow\' or \'wobble\' over time; it’s just a slightly less precise version of the original setting."}; it is injected once per forward pass. This makes weights unusually tolerant to low precision relative to other components.\n\n:note[**Typical regimes**]{content="Precision Regimes (BF16, FP8, INT4)\\\n\\\nRecall the difference between **exponent** and **mantissa:**\\\n**Exponent**: Controls the scale/range of the number (how large or small it can be). \\\n**Mantissa (or significand)**: Controls the precision/detail of the number (how accurately values are represented). \\\n\\\nIn floating-point formats: **Exponent = range, Mantissa = precision** \\\n\\\n**Example**: Representing the number 12.5 \\\nIn binary floating-point form: \\\n12.5=1.5625×23 \\\n    Mantissa (Significand): 1.5625 → this holds the precision/detail of the number. \\\n\\\n    Exponent: 3 → this tells us to scale the mantissa by 23, giving the correct range/size.\\\n\\\n**Intuition:** These represent the granularity of your ruler.\\\n    **BF16 (Training):** High resolution. Like measuring a room in millimeters to ensure the foundation is perfect.\\\n    **FP8 (Deployment):** Standard resolution. Measuring in centimeters—good enough for most furniture.\\\n    **INT4 (Flash/Mobile):** Low resolution. Measuring in inches. Much faster to calculate, but you might miss some tight fits."}\n\n* **Training:** :note[BF16]{content="**BF16 (BFloat16, Brain Floating Point 16)** is a 16-bit floating-point format designed for deep learning. It keeps the same **8-bit exponent as FP32** (single precision) but reduces the **mantissa to 7 bits**, giving it the wide dynamic range of FP32 with lower precision. This makes BF16 ideal for training large neural networks efficiently."} remains the dominant standard; FP8 is used in strictly controlled settings to manage memory overhead.\n* **Inference:** **FP8** is the current standard for high-end deployment (Blackwell/Hopper). **INT4/NF4** is standard for "Instant" or "Flash" model variants to maximize throughput.\n\n**Patterns that matter.**\n\n* :note[**Per-channel scaling**]{content="**Per-channel vs. Per-tensor Scaling**\\\n\\\n**Intuition:** This is about individual vs. group adjustments.\\\n**Analogy:** If you are editing a photo, \'Per-tensor\' is like changing the brightness of the entire image at once. \'Per-channel\' is like adjusting the brightness of the reds, greens, and blues separately. The latter prevents one very bright spot from ruining the detail in the shadows."} dominates per-tensor scaling for better error distribution.\n* **Outlier-aware methods** (AWQ/GPTQ-descendants) preserve rare but influential weight directions.\n* **Embedding tables and output projections** are consistently kept at higher precision (BF16) to maintain vocabulary fidelity.\n\n**Failure mode.** Over-quantized weights do not usually cause catastrophic failure; instead, they cause *expressive collapse* - fewer distinct activation patterns and degraded reasoning depth.\n\n---\n\n## 2. Activations\n\n**Observation.** Activation precision determines how much signal survives depth.\n\n**Key distinction.** Unlike weights, activations are dynamic. Their distributions shift across layers, prompts, sequence positions, and time. Quantization noise here *compounds layer by layer*.\n\n**Typical regimes.**\n\n* **Training:** BF16 is the industry baseline.\n* **Inference:** FP8 (specifically E4M3 or E5M2) is the current "sweet spot" for Blackwell-class hardware.\n* **Legacy:** INT8 is still common on older hardware, though it requires complex calibration compared to the more flexible FP8.\n\n**Why this is harder than weights.** Every layer consumes the output of the previous one. Small distortions in early layers become large distortions downstream, especially in long-context settings.\n\n**Emergent rule.** If reasoning depth degrades before factual accuracy, activation precision is usually the culprit.\n\n---\n\n## 3. Accumulators\n\n**Observation.** Accumulators anchor numerical stability.\n\nEvery matrix multiplication is a sequence of multiply–accumulate operations. Even when inputs and weights are low precision (FP8/INT8), the accumulation step must preserve small differences across thousands of operations.\n\n**Typical regimes.**\n\n* **Training:** FP32 accumulators remain the norm to ensure gradient stability.\n* **Inference:** FP16/BF16 accumulators; FP8 accumulation is viable primarily with hardware-assisted compensation logic in newer chips.\n\n**Practical takeaway.** If a system diverges unpredictably or "hallucinates" in a nonsensical loop, check the accumulator precision before the weight precision.\n\n---\n\n## 4. Selective Fallbacks\n\n**Observation.** No frontier model is uniformly quantized.\n\nModern systems maintain *precision maps*: layer-wise decisions about where low precision is acceptable and where it is not. While the exact maps for GPT-5.2 or Gemini 3.0 are not public, architectural research suggests consistent fallback zones.\n\n**Common fallback zones.**\n\n* First and last layers (Input embeddings/Output heads)\n* Mixture-of-Experts routing networks (to ensure stable expert selection)\n* Long-context KV caches\n* Normalization statistics\n\n**Dynamic behavior.** Precision is increasingly dynamic at runtime. **Likely Scenario:** KV caches often escalate from INT8/FP8 to FP16 as context length grows to prevent numerical drift.\n\n**Interpretation.** Fallbacks are not inefficiencies; they are *structural safeguards*.\n\n---\n\n## 5. Attention and LayerNorm\n\n**Observation.** Attention and normalization are the least precision-tolerant components in the entire stack.\n\n### Attention\n\nAttention combines dot products, exponentiation, and normalization. Small numerical errors in any step can radically change token weighting.\n\n* QK dot products usually remain in **BF16/FP16**.\n* Softmax logits are almost always computed in higher precision to prevent overflow.\n* KV caches receive special handling for long sequences (often involving dynamic rescaling).\n\n### LayerNorm / RMSNorm\n\nNormalization layers compute global statistics. Quantization here affects *every downstream operation*.\n\n* Mean/variance (or RMS) estimation is typically performed in **BF16 or FP32**.\n* **Disclaimer:** Aggressive quantization in normalization leads to variance collapse and training divergence.\n\n---\n\n## 6. Case Studies: The 2025/2026 Cohort Estimated Precision Profiles\n\nBy early 2026, the industry has moved beyond monolithic precision. The major labs employ distinct strategies based on their hardware lineage (GPU vs. TPU).\n\n### Gemini 3.0 Pro (Google)\n\n* **Architecture:** Sparse Mixture-of-Experts (MoE).\n* **Precision Strategy (Estimated):** **INT8 / BF16 Hybrid.**\n* As a TPU-native model (TPU v5/v6/v7), Gemini utilizes the TPU\'s robust BF16 paths.\n* For serving, expert weights are **likely INT8 quantized** to maximize memory bandwidth, while active calculation paths remain in BF16.\n\n\n* **The "Thinking Level":** Gemini 3 introduced "Deep Think" modes. This effectively trades latency for *computational precision* via hidden chain-of-thought tokens.\n\n### GPT-5.2 (OpenAI)\n\n* **Variants:** `instant` (latency-optimized) and `thinking` (reasoning-optimized).\n* **Precision Strategy:** **FP8 (Float8) Native.**\n* Leveraging the **NVIDIA Blackwell** infrastructure, GPT-5.2 utilizes native FP8 optimization via the Transformer Engine.\n* **GPT-5.2 Instant:** Likely utilizes aggressive **INT4 or NF4** quantization for weights to maximize token throughput for everyday tasks.\n\n\n\n### Claude Sonnet 4.5 (Anthropic)\n\n* **Focus:** Reliability, Coding, and Agentic Workflows.\n* **Precision Strategy:** **FP8 Conservative.**\n* Anthropic typically avoids ultra-aggressive quantization that risks "faithfulness."\n* FP8 serves as the "Goldilocks" zone: halving memory usage vs BF16 while preserving the subtle reasoning required for complex coding.\n\n\n\n| Feature | Gemini 3.0 Pro | GPT-5.2 | Sonnet 4.5 |\n| --- | --- | --- | --- |\n| **Inference Weights** | INT8 / BF16 (Hybrid) | FP8 / INT4 (Instant) | FP8 (Float8) |\n| **Inference Activations** | BF16 | FP8 | FP8 / BF16 |\n| **Hardware Bias** | TPU (Google) | GPU (NVIDIA) | GPU (NVIDIA) |\n\n---\n\n## 7. The Hardware Substrate (Early 2026)\n\n### The Contenders\n\n1. **Ironwood (Google TPU v7):**\n* **Status:** GA as of November 2025.\n* **Scaling:** Connects **9,216 chips** into a single pod via Optical Circuit Switches.\n* **Precision:** Heavily optimized for BF16 and INT8, delivering ~4.6 PFLOPS per chip.\n\n\n2. **Blackwell Ultra (NVIDIA B300):**\n* **Philosophy:** Memory Density.\n* **Key Stat:** **288GB HBM3e** per chip. This allows massive batch sizes on a single device.\n* **Precision:** The definitive **FP8 engine** (scaling to FP4 in specialized modes).\n\n\n\n### The Groq Factor\n\nIn a major late-2025 shift, **NVIDIA finalized a $20B deal** to license Groq’s LPU technology and bring their core talent in-house. This signals a strategic move toward specialized, deterministic inference units to solve the "latency floor" issue in general-purpose GPUs.\n\n---\n\n## Closing Synthesis\n\nAcross frontier models, a consistent ordering regarding precision tolerance remains valid:\n\n![Precision Sensitivity](/images/precision-sensitivity.png)\n\nModern "mixed precision" now encompasses three dimensions:\n\n1. **Numerical:** (FP4/FP8/BF16) tailored to the specific layer.\n2. **Temporal:** (Thinking steps) extra tokens allocated to hard problems.\n3. **Architectural:** (General GPU vs. Dedicated LPU) tailored to the workload phase.\n';export{e as default};

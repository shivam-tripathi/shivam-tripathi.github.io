const e="# Notes on Precision in Frontier Language Models\n\n\nModern frontier language models are not trained or served in a single numerical precision. They are *precision systems*: layered, conditional, hardware-aware constructions where different parts of the model intentionally live at different numerical fidelities.\n\nThis document captures working notes on how that precision is allocated - and why - across weights, activations, and the hardware landscape of early 2026. These notes attempt to extract the underlying principles that hold across OpenAI (GPT-5.2), Google DeepMind (Gemini 3.0), and Anthropic (Sonnet 4.5) systems.\n\n---\n\n## 1. Weights\n\n**Observation.** Weights are the most aggressively :note[quantized]{content=\"**Intuition:** This is the art of reducing resolution.\\\n\\\n**Analogy:** Imagine describing a color. Instead of using a complex code like 'Midnight Navy Blue,' you just call it 'Dark Blue.' You save 'memory' in your brain, and while you lose some nuance, the person you’re talking to still gets the general idea.\\\n\\\nIn practice it happens by reducing the 'precision' of the number. For example, FP16 (floating point using 16 bits) if replaced by INT8 (integer using 8 bits) reduces the overall memory requirements.\"} component in modern LLMs.\n\n**Why this works.** During inference, :note[weights are static]{content=\"**Intuition:** Think of weights as the fixed settings on a massive mixing board. They represent the strength of connections between neurons.\\\n\\\n**The 'Static' Principle:** Because they don't change while the model is answering a prompt (inference), any error we introduce by 'rounding' the numbers is fixed. It doesn't 'grow' or 'wobble' over time; it’s just a slightly less precise version of the original setting.\"}; it is injected once per forward pass. This makes weights unusually tolerant to low precision relative to other components.\n\n:note[**Typical regimes**]{content=\"Precision Regimes (BF16, FP8, INT4)\\\n\\\nRecall the difference between **exponent** and **mantissa:**\\\n**Exponent**: Controls the scale/range of the number (how large or small it can be). \\\n**Mantissa (or significand)**: Controls the precision/detail of the number (how accurately values are represented). \\\n\\\nIn floating-point formats: **Exponent = range, Mantissa = precision** \\\n\\\n**Example**: Representing the number 12.5 \\\nIn binary floating-point form: \\\n12.5=1.5625×23 \\\n    Mantissa (Significand): 1.5625 → this holds the precision/detail of the number. \\\n\\\n    Exponent: 3 → this tells us to scale the mantissa by 23, giving the correct range/size.\\\n\\\n**Intuition:** These represent the granularity of your ruler.\\\n    **BF16 (Training):** High resolution. Like measuring a room in millimeters to ensure the foundation is perfect.\\\n    **FP8 (Deployment):** Standard resolution. Measuring in centimeters—good enough for most furniture.\\\n    **INT4 (Flash/Mobile):** Low resolution. Measuring in inches. Much faster to calculate, but you might miss some tight fits.\"}\n\n* **Training:** :note[BF16]{content=\"**BF16 (BFloat16, Brain Floating Point 16)** is a 16-bit floating-point format designed for deep learning. It keeps the same **8-bit exponent as FP32** (single precision) but reduces the **mantissa to 7 bits**, giving it the wide dynamic range of FP32 with lower precision. This makes BF16 ideal for training large neural networks efficiently.\"} remains the dominant standard; FP8 is used in strictly controlled settings to manage memory overhead.\n* **Inference:** **FP8** is the current standard for high-end deployment (Blackwell/Hopper). **INT4/NF4** is standard for \"Instant\" or \"Flash\" model variants to maximize throughput.\n\n**Patterns that matter.**\n\n* :note[**Per-channel scaling**]{content=\"**Per-channel vs. Per-tensor Scaling**\\\n\\\n**Intuition:** This is about individual vs. group adjustments.\\\n**Analogy:** If you are editing a photo, 'Per-tensor' is like changing the brightness of the entire image at once. 'Per-channel' is like adjusting the brightness of the reds, greens, and blues separately. The latter prevents one very bright spot from ruining the detail in the shadows.\\\n\\\n**Per-Tensor Scaling**\\\n*Definition:* Use one single scale factor for the entire weight matrix (all rows/channels).\\\n*Analogy:* Imagine adjusting the brightness of an entire photo with one slider. If one part of the photo is very bright and another is very dark, the dark areas may get lost because the slider has to accommodate the brightest part.\\\n*Effect:* Efficient, but small weights in some channels can get 'crushed' toward zero because the scale is dominated by the largest values.\\\n\\\n**Per-Channel Scaling**\\\n*Definition:* Use a different scale factor for each output channel (row) in the weight matrix.\\\n*Analogy:* Instead of one brightness slider for the whole photo, you give each section its own slider. Now even the darker regions can be adjusted independently so details aren’t lost.\\\n*Effect:* Preserves sensitivity of neurons with smaller weights, ensuring rare but important features aren’t wiped out during quantization.\"} dominates per-tensor scaling for better error distribution.\n* :note[**Outlier-aware methods** (AWQ/GPTQ-descendants)]{content=\"**AWQ (Activation-aware Weight Quantization)** and **GPTQ (Generalized Post-Training Quantization)** are two popular post-training quantization methods for compressing large language models (LLMs) by reducing precision of weights. GPTQ focuses on minimizing quantization error in weights, while AWQ balances weight quantization with activation sensitivity, often yielding better accuracy at low bit-widths\\\n\\\n**Outlier-Aware Methods (AWQ/GPTQ)**\\\n    *Intuition:* Identifying the 'VIP' numbers.\\\n    *Analogy:* In a gourmet recipe, 1 gram of saffron is more important than 1 gram of flour. Outlier-aware methods ensure that the most 'influential' weights (the saffron) are kept at high precision, while the 'filler' weights (the flour) can be rounded down aggressively.\"} preserve rare but influential weight directions.\n* :note[**Embedding tables and output projections**]{content=\"**Vocabulary Fidelity (Embeddings & Projections)**\\\n\\\n    **Intuition:** Protecting the 'Dictionary.'\\\n\\\n    **Analogy:** If you blur a photo, you can still tell it’s a forest. But if you blur the letters in a dictionary, 'cat' might start looking like 'eat.' We keep the input (embeddings) and output (projections) at high precision so the model doesn't confuse similar-sounding words.\"} are consistently kept at higher precision (BF16) to maintain vocabulary fidelity.\n\n**Failure mode.** Over-quantized weights do not usually cause catastrophic failure; instead, they cause :note[*expressive collapse*]{content=\"**Expressive Collapse (The Failure Mode)**\\\n\\\n    **Intuition:** The 'Loss of Nuance.'\\\n\\\n    **Analogy:** A professional chef with 50 spices can make complex, layered dishes. If you take away 45 of those spices, they can still cook a meal, but everything starts tasting 'salty' or 'sweet' without any depth. The model still talks, but its reasoning becomes repetitive and shallow.\"} - fewer distinct activation patterns and degraded reasoning depth.\n\n---\n\n## 2. Activations\n\n**Observation.** Activation :note[precision]{content=\"**1. Activations as 'Dynamic Signals'**\\\n\\\n**Intuition:** Think of activations as the 'Live Current.' They only exist when the model is actually 'thinking' about a specific prompt.\\\n\\\n**The 'Dynamic' Principle:** Unlike weights, which stay the same for every user, activations change based on every word you type. They are the fluid, shifting state of the model’s internal logic at any given millisecond.\"} determines how :note[much signal survives depth]{content=\"**2. Signal Survival & Depth**\\\n\\\n**Intuition:** This is the 'Signal-to-Noise Ratio.'\\\n\\\n**Analogy:** Imagine a relay race where runners pass a bucket of water. The 'precision' is how steady the runners' hands are. If the precision is too low, they splash a bit of water at every handoff. By the end of a long race (a 'deep' model), the bucket might be empty.\"}.\n\n**Key distinction.** Unlike weights, activations are dynamic. Their distributions shift across layers, prompts, sequence positions, and time. Quantization noise here :note[*compounds layer by layer*]{content=\"**3. Compounding Noise (The 'Telephone' Effect)**\\\n\\\n**Intuition:** Errors in activations aren't 'one-and-done'; they snowball.\\\n\\\n**The Principle:** Because Layer 2's input is Layer 1's output, a 1% rounding error in Layer 1 is multiplied by the weights of Layer 2, then rounded again. By Layer 96, that tiny 1% error could have mutated into a completely different 'thought'.\"}.\n\n**Typical regimes.**\n\n* **Training:** BF16 is the industry baseline.\n* **Inference:** :note[FP8 (specifically E4M3 or E5M2)]{content=\"**4. FP8: The 'Sweet Spot' (E4M3 vs. E5M2)**\\\n\\\nE here is exponent and M is mantissa.\\\n\\\n**Intuition:** These are different 'Digital Ruler Designs.'\\\n\\\n    **E4M3:** More focus on precision (the decimal points).\\\n\\\n    **E5M2:** More focus on range (how big or small the numbers can get).\\\n\\\n**The Logic:** Modern hardware uses FP8 because it’s the 'Goldilocks' zone—small enough to be lightning-fast, but precise enough that the 'water' doesn't splash out of the bucket too quickly.\"} is the current \"sweet spot\" for Blackwell-class hardware.\n* **Legacy:** INT8 is still common on older hardware, though it requires complex calibration compared to the more flexible FP8.\n\n**Why this is harder than weights.** Every layer consumes the output of the previous one. Small distortions in early layers become large distortions downstream, especially in long-context settings.\n\n**Emergent rule.** If :note[reasoning depth degrades before factual accuracy]{content=\"**5. Reasoning Depth vs. Factual Accuracy**\\\n\\\n**Intuition:** The 'Drunken Scholar' effect.\\\n\\\n**Analogy:** If a model has high-precision weights but low-precision activations, it's like a genius who has had too much to drink. They still know all the facts (Weights), but they lose the ability to follow a complex logical chain (Activations). They can tell you who George Washington was, but they can't solve a complex riddle about him.\"}, activation precision is usually the culprit.\n\n---\n\n## 3. Accumulators\n\n**Observation.** Accumulators :note[anchor numerical stability]{content=\"**Numerical Stability (Anchoring the Math)**\\\n\\\n**Intuition:** Preventing 'Math Erasure.'\\\n\\\n**Analogy:** Imagine you are weighing a giant ship. You need a scale that can sense the weight of a single passenger (high precision) but also hold the weight of the entire hull (high range). If your scale is too 'coarse,' the weight of the passengers is simply rounded to zero, and your final calculation is 'unstable' or inaccurate.\"}.\n\nEvery matrix multiplication is a sequence of :note[multiply–accumulate operations]{content=\"**The Accumulator as the 'Running Total'**\\\n\\\n**Intuition:** Think of it as a high-capacity bucket at the end of a long assembly line.\\\n\\\n**The Principle:** In matrix multiplication, the hardware performs a 'Multiply-Accumulate' (MAC) operation. It multiplies two numbers (Weight × Activation) and then adds that result to a running total. The accumulator is the 'ledger' where that total is kept.\"}. Even when inputs and weights are low precision (FP8/INT8), the accumulation step must :note[preserve small differences]{content=\"**Precision Hierarchy (Why Accumulators need more 'bits')**\\\n\\\n**Intuition:** The 'Hierarchy of Detail.'\\\n\\\n**The Logic:**\\\n\\\n    **Inputs (FP8/INT8):** The ingredients.\\\n\\\n    **Accumulator (FP32/FP16):** The final dish.\\\n\\\n**Why it matters:** You can use cheap, small 'measuring cups' (low precision inputs), but your 'mixing bowl' (the accumulator) must be large and precise. If the bowl is too small, the ingredients 'overflow' (overflow error) or the fine spices vanish into the cracks (underflow/rounding error).\"} across thousands of operations.\n\n**Typical regimes.**\n\n* **Training:** FP32 accumulators remain the norm to ensure gradient stability.\n* **Inference:** FP16/BF16 accumulators; FP8 accumulation is viable primarily with hardware-assisted compensation logic in newer chips.\n\n**Practical takeaway.** If a system diverges unpredictably or :note[\"hallucinates\" in a nonsensical loop]{content=\"**Failure Mode: The 'Nonsensical Loop'**\\\n\\\n**Intuition:** The 'Broken Compass.'\\\n\\\n**Analogy:** If the accumulator isn't precise enough, the model’s 'math' starts to lose its direction. Instead of settling on a clear answer, the numbers might start bouncing back and forth between two wrong values. This is why a model might start repeating the same word over and over—it’s 'lost' in a mathematical loop because it can no longer see the small differences that would lead it to the next thought.\"}, check the accumulator precision before the weight precision.\n\n---\n\n## 4. Selective Fallbacks\n\n**Observation.** No frontier model is uniformly quantized.\n\nModern systems maintain *precision maps*: layer-wise decisions about where low precision is acceptable and where it is not. While the exact maps for GPT-5.2 or Gemini 3.0 are not public, architectural research suggests consistent fallback zones.\n\n**Common fallback zones.**\n\n* First and last layers (Input embeddings/Output heads)\n* Mixture-of-Experts routing networks (to ensure stable expert selection)\n* Long-context KV caches\n* Normalization statistics\n\n**Dynamic behavior.** Precision is increasingly dynamic at runtime. **Likely Scenario:** KV caches often escalate from INT8/FP8 to FP16 as context length grows to prevent numerical drift.\n\n**Interpretation.** Fallbacks are not inefficiencies; they are *structural safeguards*.\n\n---\n\n## 5. Attention and LayerNorm\n\n**Observation.** Attention and normalization are the least precision-tolerant components in the entire stack.\n\n### Attention\n\nAttention combines dot products, exponentiation, and normalization. Small numerical errors in any step can radically change token weighting.\n\n* QK dot products usually remain in **BF16/FP16**.\n* Softmax logits are almost always computed in higher precision to prevent overflow.\n* KV caches receive special handling for long sequences (often involving dynamic rescaling).\n\n### LayerNorm / RMSNorm\n\nNormalization layers compute global statistics. Quantization here affects *every downstream operation*.\n\n* Mean/variance (or RMS) estimation is typically performed in **BF16 or FP32**.\n* **Disclaimer:** Aggressive quantization in normalization leads to variance collapse and training divergence.\n\n---\n\n## 6. Case Studies: The 2025/2026 Cohort Estimated Precision Profiles\n\nBy early 2026, the industry has moved beyond monolithic precision. The major labs employ distinct strategies based on their hardware lineage (GPU vs. TPU).\n\n### Gemini 3.0 Pro (Google)\n\n* **Architecture:** Sparse Mixture-of-Experts (MoE).\n* **Precision Strategy (Estimated):** **INT8 / BF16 Hybrid.**\n* As a TPU-native model (TPU v5/v6/v7), Gemini utilizes the TPU's robust BF16 paths.\n* For serving, expert weights are **likely INT8 quantized** to maximize memory bandwidth, while active calculation paths remain in BF16.\n\n\n* **The \"Thinking Level\":** Gemini 3 introduced \"Deep Think\" modes. This effectively trades latency for *computational precision* via hidden chain-of-thought tokens.\n\n### GPT-5.2 (OpenAI)\n\n* **Variants:** `instant` (latency-optimized) and `thinking` (reasoning-optimized).\n* **Precision Strategy:** **FP8 (Float8) Native.**\n* Leveraging the **NVIDIA Blackwell** infrastructure, GPT-5.2 utilizes native FP8 optimization via the Transformer Engine.\n* **GPT-5.2 Instant:** Likely utilizes aggressive **INT4 or NF4** quantization for weights to maximize token throughput for everyday tasks.\n\n\n\n### Claude Sonnet 4.5 (Anthropic)\n\n* **Focus:** Reliability, Coding, and Agentic Workflows.\n* **Precision Strategy:** **FP8 Conservative.**\n* Anthropic typically avoids ultra-aggressive quantization that risks \"faithfulness.\"\n* FP8 serves as the \"Goldilocks\" zone: halving memory usage vs BF16 while preserving the subtle reasoning required for complex coding.\n\n\n\n| Feature | Gemini 3.0 Pro | GPT-5.2 | Sonnet 4.5 |\n| --- | --- | --- | --- |\n| **Inference Weights** | INT8 / BF16 (Hybrid) | FP8 / INT4 (Instant) | FP8 (Float8) |\n| **Inference Activations** | BF16 | FP8 | FP8 / BF16 |\n| **Hardware Bias** | TPU (Google) | GPU (NVIDIA) | GPU (NVIDIA) |\n\n---\n\n## 7. The Hardware Substrate (Early 2026)\n\n### The Contenders\n\n1. **Ironwood (Google TPU v7):**\n* **Status:** GA as of November 2025.\n* **Scaling:** Connects **9,216 chips** into a single pod via Optical Circuit Switches.\n* **Precision:** Heavily optimized for BF16 and INT8, delivering ~4.6 PFLOPS per chip.\n\n\n2. **Blackwell Ultra (NVIDIA B300):**\n* **Philosophy:** Memory Density.\n* **Key Stat:** **288GB HBM3e** per chip. This allows massive batch sizes on a single device.\n* **Precision:** The definitive **FP8 engine** (scaling to FP4 in specialized modes).\n\n\n\n### The Groq Factor\n\nIn a major late-2025 shift, **NVIDIA finalized a $20B deal** to license Groq’s LPU technology and bring their core talent in-house. This signals a strategic move toward specialized, deterministic inference units to solve the \"latency floor\" issue in general-purpose GPUs.\n\n---\n\n## Closing Synthesis\n\nAcross frontier models, a consistent ordering regarding precision tolerance remains valid:\n\n![Precision Sensitivity](/images/precision-sensitivity.png)\n\nModern \"mixed precision\" now encompasses three dimensions:\n\n1. **Numerical:** (FP4/FP8/BF16) tailored to the specific layer.\n2. **Temporal:** (Thinking steps) extra tokens allocated to hard problems.\n3. **Architectural:** (General GPU vs. Dedicated LPU) tailored to the workload phase.\n";export{e as default};

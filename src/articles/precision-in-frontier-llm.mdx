# Notes on Precision in Frontier Language Models


Modern frontier language models are not trained or served in a single numerical precision. They are *precision systems*: layered, conditional, hardware-aware constructions where different parts of the model intentionally live at different numerical fidelities.

This document captures working notes on how that precision is allocated - and why - across weights, activations, and the hardware landscape of early 2026. These notes attempt to extract the underlying principles that hold across OpenAI (GPT-5.2), Google DeepMind (Gemini 3.0), and Anthropic (Sonnet 4.5) systems.

---

## 1. Case Studies: The 2025/2026 Cohort Estimated Precision Profiles

By early 2026, the industry has moved beyond monolithic precision. The major labs employ distinct strategies based on their hardware lineage (GPU vs. TPU).

### Gemini 3.0 Pro (Google)

* **Architecture:** :note[Sparse Mixture-of-Experts (MoE).]{content="The Mixture of Experts (MoE) is a machine learning architecture that divides a complex problem into simpler sub-problems handled by specialized sub-networks called 'experts.' Introduced in the early 1990s (e.g., the 1991 paper 'Adaptive Mixture of Local Experts'), it combines multiple neural networks (experts) with a gating network (or router) that decides how to weigh or route inputs to the experts. \
\
Sparse MoE is the modern variant dominant in large language models (LLMs). It introduces sparsity: only a small subset of experts (e.g., top-2 or top-k) activate per input, drastically reducing computation while maintaining a large total parameter count."}
* **Precision Strategy (Estimated):** **INT8 / BF16 Hybrid.**
* As a TPU-native model (TPU v5/v6/v7), Gemini utilizes the TPU's robust BF16 paths.
* For serving, expert weights are **likely INT8 quantized** to maximize memory bandwidth, while active calculation paths remain in BF16.


* **The "Thinking Level":** Gemini 3 introduced "Deep Think" modes. This effectively trades latency for *computational precision* via hidden chain-of-thought tokens.

### GPT-5.2 (OpenAI)

* **Variants:** `instant` (latency-optimized) and `thinking` (reasoning-optimized).
* **Precision Strategy:** **FP8 (Float8) Native.**
* Leveraging the **NVIDIA Blackwell** infrastructure, GPT-5.2 utilizes native FP8 optimization via the Transformer Engine.
* **GPT-5.2 Instant:** Likely utilizes aggressive **INT4 or NF4** quantization for weights to maximize token throughput for everyday tasks.


### Claude Sonnet 4.5 (Anthropic)

* **Focus:** Reliability, Coding, and Agentic Workflows.
* **Precision Strategy:** **FP8 Conservative.**
* Anthropic typically avoids ultra-aggressive quantization that risks "faithfulness."
* FP8 serves as the "Goldilocks" zone: halving memory usage vs BF16 while preserving the subtle reasoning required for complex coding.



| Feature | Gemini 3.0 Pro | GPT-5.2 | Sonnet 4.5 |
| --- | --- | --- | --- |
| **Inference Weights** | INT8 / BF16 (Hybrid) | FP8 / INT4 (Instant) | FP8 (Float8) |
| **Inference Activations** | BF16 | FP8 | FP8 / BF16 |
| **Hardware Bias** | TPU (Google) | GPU (NVIDIA) | GPU (NVIDIA) |

---

## 2. The Hardware Substrate (Early 2026)

### The Contenders

1. **Ironwood (Google TPU v7):**
* **Status:** GA as of November 2025.
* **Scaling:** Connects **9,216 chips** into a single pod via Optical Circuit Switches.
* **Precision:** Heavily optimized for BF16 and INT8, delivering ~4.6 PFLOPS per chip.


2. **Blackwell Ultra (NVIDIA B300):**
* **Philosophy:** Memory Density.
* **Key Stat:** **288GB HBM3e** per chip. This allows massive batch sizes on a single device.
* **Precision:** The definitive **FP8 engine** (scaling to FP4 in specialized modes).


### The Groq Factor

In a major late-2025 shift, **NVIDIA finalized a $20B deal** to license Groq’s LPU technology and bring their core talent in-house. This signals a strategic move toward specialized, deterministic inference units to solve the "latency floor" issue in general-purpose GPUs.

## 3. Quantization

### 1. Weights

**Observation.** Weights are the most aggressively :note[quantized]{content="In the context of machine learning and neural networks (like those in large language models), quantization refers to the process of reducing the precision of numerical values - specifically the model's weights - to make the model more efficient. Weights are the learned parameters in a neural network that determine how inputs are transformed into outputs. They are typically stored as high-precision floating-point numbers (e.g., 32-bit floats, or FP32) during training.\
\
**Analogy:** Imagine describing a color. Instead of using a complex code like 'Midnight Navy Blue,' you just call it 'Dark Blue.' You save 'memory' in your brain, and while you lose some nuance, the person you’re talking to still gets the general idea."} component in modern LLMs.

**Why this works.** During :note[inference]{content="The phase where the trained model is used to generate responses (e.g., answering a prompt). This contrasts with training, where weights are repeatedly updated based on errors."}, :note[weights are static]{content="Because they don't change while the model is answering a prompt (inference), any error we introduce by 'rounding' the numbers is fixed. It doesn't 'grow' or 'wobble' over time; it’s just a slightly less precise version of the original setting."}; it is injected once per forward pass. This makes weights unusually tolerant to low precision relative to other components.

:note[**Typical regimes**]{content="**Precision Regimes in Neural Networks: BF16, FP8, and INT4**\
\
When training and deploying large neural networks, we use different numerical formats to represent weights and activations. These formats trade off between range (how large or small numbers can be) and precision (how finely we can distinguish between close values). The key components in floating-point formats are:\
\
**Exponent:** Determines the range or scale of the number (how big or small it can get).\
**Mantissa (or significand):** Determines the precision or level of detail (how many significant digits we have).\
\
In short: Exponent = range, Mantissa = precision.\
Example: Representing 12.5 in Binary Floating-Point \
12.5 in decimal = 1.5625 × 2³ in scientific notation. \
\
Mantissa: 1.5625 carries the precise digits (the 'detail').\
Exponent: 3 shifts the binary point (controls the magnitude/range).\
\
Different formats allocate a different number of bits to the exponent and mantissa, leading to distinct strengths.\
\
**Intuition:** These represent the granularity of your ruler.\
    **BF16 (Training):** High resolution. Like measuring a room in millimeters to ensure the foundation is perfect.\
    **FP8 (Deployment):** Standard resolution. Measuring in centimeters-good enough for most furniture.\
    **INT4 (Flash/Mobile):** Low resolution. Measuring in inches. Much faster to calculate, but you might miss some tight fits."}

* **Training:** :note[BF16]{content="**BF16 (BFloat16, Brain Floating Point 16)** is a 16-bit floating-point format designed for deep learning. It keeps the same **8-bit exponent as FP32** (single precision) but reduces the **mantissa to 7 bits**, giving it the wide dynamic range of FP32 with lower precision. This makes BF16 ideal for training large neural networks efficiently."} remains the dominant standard; FP8 is used in strictly controlled settings to manage memory overhead.
* **Inference:** **FP8** is the current standard for high-end deployment :note[(Blackwell/Hopper)]{content="NVIDIA's Hopper and Blackwell are successive generations of data-center GPU architectures designed primarily for AI training, inference, high-performance computing (HPC), and large-scale workloads like large language models (LLMs). Hopper (2022) powered the H100 and H200 GPUs, while Blackwell (announced 2024, widely available in 2025) powers the B100, B200, and B300 series. Blackwell represents a major leap forward, especially for the era of trillion-parameter models and agentic AI.\
\
Named after pioneering figures - Hopper after computer scientist Grace Hopper, and Blackwell after mathematician David Blackwell - these architectures build on NVIDIA's lineage (e.g., Ampere → Hopper → Blackwell), prioritizing tensor operations, high-bandwidth memory, and interconnects for cluster-scale performance."}. **INT4/NF4** is standard for "Instant" or "Flash" model variants to maximize throughput.

**Failure mode.** Over-quantized weights do not usually cause :note[catastrophic failure]{content="In context of weight quantization for large language models, catastrophic failure refers to a sudden, severe, and often irreversible breakdown in model behavior - where the model stops producing coherent, meaningful, or even grammatically valid outputs altogether."}; instead, they cause :note[*expressive collapse*]{content="**Expressive Collapse (The Failure Mode)** means the 'Loss of Nuance.'\
\
A good analogy here is - suppose a professional chef with 50 spices can make complex, layered dishes. If you take away 45 of those spices, they can still cook a meal, but everything starts tasting 'salty' or 'sweet' without any depth. The model still talks, but its reasoning becomes repetitive and shallow."} - fewer distinct activation patterns and degraded reasoning depth.


### 2. Activations

**Observation.** Activation :note[precision]{content="Activations can be thought of as 'Dynamic Signals' or 'Live Current.' They only exist when the model is actually 'thinking' about a specific prompt.\
\
Unlike weights, which stay the same for every user, activations change based on every word you type. They are the fluid, shifting state of the model’s internal logic at any given millisecond."} determines how :note[much signal survives depth]{content="This is the 'Signal-to-Noise Ratio' because of 'depth'.\
Imagine a relay race where runners pass a bucket of water. The 'precision' is how steady the runners' hands are. If the precision is too low, they splash a bit of water at every handoff. By the end of a long race (a 'deep' model), the bucket might be empty."}.

**Key distinction.** Unlike weights, activations are dynamic. Their distributions shift across layers, prompts, sequence positions, and time. Quantization noise here :note[*compounds layer by layer*]{content="Errors in activations aren't 'one-and-done'; they snowball.\
Because Layer 2's input is Layer 1's output, a 1% rounding error in Layer 1 is multiplied by the weights of Layer 2, then rounded again. By Layer 96, that tiny 1% error could have mutated into a completely different 'thought'."}.

**Typical regimes.**

* **Training:** BF16 is the industry baseline.
* **Inference:** :note[FP8 (specifically E4M3 or E5M2)]{content="**Quick Recap: FP8 Formats**\
\
**E4M3**: 4 exponent bits, 3 mantissa bits. More focus on precision (the decimal points). \
**E5M2**: 5 exponent bits, 2 mantissa bits.  More focus on range (how big or small the numbers can get). \
\
**The Logic:** Modern hardware uses FP8 because it’s the 'Goldilocks' zone-small enough to be lightning-fast, but precise enough that the 'water' doesn't splash out of the bucket too quickly.\
\
But as of early 2026, FP8 is no longer the undisputed 'sweet spot' for inference on NVIDIA Blackwell GPUs. NVIDIA's proprietary NVFP4 (a hardware-accelerated 4-bit floating-point format) has emerged as the new optimal balance for most LLM inference workloads on Blackwell (B100/B200/B300 series and consumer RTX 50-series)."} is the most common precision type.
* **Legacy:** INT8 is still common on older hardware, though it requires complex calibration compared to the more flexible FP8.

**Why this is harder than weights.** Every layer consumes the output of the previous one. Small distortions in early layers become large distortions downstream, especially in long-context settings.

**Emergent rule.** If :note[reasoning depth degrades before factual accuracy]{content="Reasoning Depth vs. Factual Accuracy. If a model has high-precision weights but low-precision activations, it's like a genius who has had too much to drink. They still know all the facts (Weights), but they lose the ability to follow a complex logical chain (Activations). They can tell you who George Washington was, but they can't solve a complex riddle about him."}, activation precision is usually the culprit.


### 3. :note[Accumulators]{content="**The Accumulator as the 'Running Total'**: Accumulators act as a high-capacity bucket at the end of a long assembly line. In matrix multiplication, the hardware performs a 'Multiply-Accumulate' (MAC) operation. It multiplies two numbers (Weight × Activation) and then adds that result to a running total. The accumulator is the 'ledger' where that total is kept. It is the internal registers or temporary storage used during matrix multiply-accumulate (MMA or GEMM) operations to sum up partial products before producing the final output."}

**Observation.** Accumulators :note[anchor numerical stability]{content="**Numerical Stability (Anchoring the Math)**\
Imagine you are weighing a giant ship. You need a scale that can sense the weight of a single passenger (high precision) but also hold the weight of the entire hull (high range). If your scale is too 'coarse,' the weight of the passengers is simply rounded to zero, and your final calculation is 'unstable' or inaccurate."}.

Every matrix multiplication is a sequence of multiply–accumulate operations. Even when inputs and weights are low precision (FP8/INT8), the accumulation step must :note[preserve small differences]{content="**Why Accumulators need more 'bits'?** Because of the possibility of overflows, given they store the accumulated value.\
Inputs (FP8/INT8) are the ingredients. Accumulator (FP32/FP16) is the final dish. You can use cheap, small 'measuring cups' (low precision inputs), but your 'mixing bowl' (the accumulator) must be large and precise. If the bowl is too small, the ingredients 'overflow' (overflow error) or the fine spices vanish into the cracks (underflow/rounding error)."} across thousands of operations.

**Typical regimes.**

* **Training:** FP32 accumulators remain the norm to ensure gradient stability.
* **Inference:** FP16/BF16 accumulators; FP8 accumulation is viable primarily with hardware-assisted compensation logic in newer chips.

**Practical takeaway.** If a system diverges unpredictably or :note["hallucinates" in a nonsensical loop]{content="If the accumulator isn't precise enough, the model’s 'math' starts to lose its direction. Instead of settling on a clear answer, the numbers might start bouncing back and forth between two wrong values. This is why a model might start repeating the same word over and over-it’s 'lost' in a mathematical loop because it can no longer see the small differences that would lead it to the next thought."}, check the accumulator precision before the weight precision.

### 4. Selective Fallbacks

**Observation.** No frontier model is uniformly quantized.

Modern systems maintain *precision maps*: layer-wise decisions about where low precision is acceptable and where it is not. While the exact maps for GPT-5.2 or Gemini 3.0 are not public, architectural research suggests consistent fallback zones.

**Common fallback zones.**

* First and last layers (Input embeddings/Output heads)
* Mixture-of-Experts routing networks (to ensure stable expert selection)
* Long-context KV caches
* Normalization statistics

**Dynamic behavior.** Precision is increasingly dynamic at runtime. **Likely Scenario:** KV caches often escalate from INT8/FP8 to FP16 as context length grows to prevent numerical drift.

**Interpretation.** Fallbacks are not inefficiencies; they are *structural safeguards*.

### 5. Attention and LayerNorm

**Observation.** :note[Attention and normalization]{content="**Attention as 'The Selective Spotlight'**\
\
**Intuition:** Attention is how the model decides which words matter most.\
\
**The Fragility Principle:** Attention involves Dot Products (matching Query to Key) followed by Softmax.\
\
**Analogy:** Imagine a talent show where judges give scores. If the scoring is too 'blunt' (low precision), a 9.1 and a 9.4 both become a '9.' But the Softmax function is like a hyperbolic fan-it takes those scores and turns them into 'Who gets the most screen time?' A tiny difference in score can be the difference between a word getting 90% of the model's focus or only 10%."} are the least precision-tolerant components in the entire stack.

### Attention

Attention combines dot products, exponentiation, and normalization. Small numerical errors in any step can :note[radically change token weighting]{content="**The Softmax 'Magnifier'**\
\
**Intuition:** Softmax turns raw scores into probabilities.\
\
**The Math:** It uses exponentiation: ex.\
\
**Why it's High-Stakes:** Because it uses powers of e, a small change in the input (x) leads to an exponential change in the output. If you round the input too early, the 'winner' of the attention spotlight changes completely, leading the model to hallucinate or lose the thread of the sentence."}.

* QK dot products usually remain in **BF16/FP16**.
* Softmax logits are almost always computed in higher precision to prevent overflow.
* :note[KV caches]{content="**KV Cache (The Model's 'Working Memory')**\
\
**Intuition:** The 'Bookmark.'\
\
**The Principle:** To save time, LLMs remember the 'Key' and 'Value' of every previous word so they don't have to re-calculate them.\
\
**The Challenge:** In long conversations, this 'notebook' gets massive. If you use messy handwriting (low precision) to save space, by page 50, the model can't remember if the 'he' in the sentence refers to the hero or the villain."} receive special handling for long sequences (often involving dynamic rescaling).

### LayerNorm / RMSNorm

:note[Normalization layers]{content="**Normalization (LayerNorm/RMSNorm)**\
\
**Intuition:** The 'Auto-Leveler.'\
\
**Analogy:** Imagine a group of people shouting at different volumes. Normalization calculates the average volume and 'scales' everyone so they can be heard clearly without blowing out the speakers.\
\
**The Risk:** Normalization relies on Global Statistics (mean and variance). If your 'math' for the average is even slightly off, you scale the entire layer incorrectly. It's like a photographer setting the white balance for a whole wedding-if they get it slightly wrong, every single photo comes out blue."} compute global statistics. Quantization here affects *every downstream operation*.

* Mean/variance (or RMS) estimation is typically performed in **BF16 or FP32**.
* **Disclaimer:** Aggressive quantization in normalization leads to :note[variance collapse]{content="**Variance Collapse (The Failure Mode)**\
\
**Intuition:** 'Mathematical Flattening.'\
\
**Analogy:** If the normalization becomes too imprecise, it stops being able to distinguish between 'loud' and 'quiet' signals. Eventually, every signal starts looking exactly the same. The model's internal 'world' loses all its features and becomes a flat, gray void where no unique thoughts can survive."} and training divergence.

---

## Closing Synthesis

Across frontier models, a consistent ordering regarding precision tolerance remains valid:

![Precision Sensitivity](/images/precision-sensitivity.png)

Modern "mixed precision" now encompasses three dimensions:

1. **Numerical:** (FP4/FP8/BF16) tailored to the specific layer.
2. **Temporal:** (Thinking steps) extra tokens allocated to hard problems.
3. **Architectural:** (General GPU vs. Dedicated LPU) tailored to the workload phase.

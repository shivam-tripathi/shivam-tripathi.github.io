# Notes on Precision in Frontier Language Models


Modern frontier language models are not trained or served in a single numerical precision. They are *precision systems*: layered, conditional, hardware-aware constructions where different parts of the model intentionally live at different numerical fidelities.

This document captures working notes on how that precision is allocated - and why - across weights, activations, and the hardware landscape of early 2026. These notes attempt to extract the underlying principles that hold across OpenAI (GPT-5.2), Google DeepMind (Gemini 3.0), and Anthropic (Sonnet 4.5) systems.

---

## 1. Weights

**Observation.** Weights are the most aggressively quantized component in modern LLMs.

**Why this works.** During inference, weights are static. Quantization error introduced at load time does not compound temporally; it is injected once per forward pass. This makes weights unusually tolerant to low precision relative to other components.

**Typical regimes**

* **Training:** BF16 remains the dominant standard; FP8 is used in strictly controlled settings to manage memory overhead.
* **Inference:** **FP8** is the current standard for high-end deployment (Blackwell/Hopper). **INT4/NF4** is standard for "Instant" or "Flash" model variants to maximize throughput.

**Patterns that matter.**

* **Per-channel scaling** dominates per-tensor scaling for better error distribution.
* **Outlier-aware methods** (AWQ/GPTQ-descendants) preserve rare but influential weight directions.
* **Embedding tables and output projections** are consistently kept at higher precision (BF16) to maintain vocabulary fidelity.

**Failure mode.** Over-quantized weights do not usually cause catastrophic failure; instead, they cause *expressive collapse* - fewer distinct activation patterns and degraded reasoning depth.

---

## 2. Activations

**Observation.** Activation precision determines how much signal survives depth.

**Key distinction.** Unlike weights, activations are dynamic. Their distributions shift across layers, prompts, sequence positions, and time. Quantization noise here *compounds layer by layer*.

**Typical regimes.**

* **Training:** BF16 is the industry baseline.
* **Inference:** FP8 (specifically E4M3 or E5M2) is the current "sweet spot" for Blackwell-class hardware.
* **Legacy:** INT8 is still common on older hardware, though it requires complex calibration compared to the more flexible FP8.

**Why this is harder than weights.** Every layer consumes the output of the previous one. Small distortions in early layers become large distortions downstream, especially in long-context settings.

**Emergent rule.** If reasoning depth degrades before factual accuracy, activation precision is usually the culprit.

---

## 3. Accumulators

**Observation.** Accumulators anchor numerical stability.

Every matrix multiplication is a sequence of multiply–accumulate operations. Even when inputs and weights are low precision (FP8/INT8), the accumulation step must preserve small differences across thousands of operations.

**Typical regimes.**

* **Training:** FP32 accumulators remain the norm to ensure gradient stability.
* **Inference:** FP16/BF16 accumulators; FP8 accumulation is viable primarily with hardware-assisted compensation logic in newer chips.

**Practical takeaway.** If a system diverges unpredictably or "hallucinates" in a nonsensical loop, check the accumulator precision before the weight precision.

---

## 4. Selective Fallbacks

**Observation.** No frontier model is uniformly quantized.

Modern systems maintain *precision maps*: layer-wise decisions about where low precision is acceptable and where it is not. While the exact maps for GPT-5.2 or Gemini 3.0 are not public, architectural research suggests consistent fallback zones.

**Common fallback zones.**

* First and last layers (Input embeddings/Output heads)
* Mixture-of-Experts routing networks (to ensure stable expert selection)
* Long-context KV caches
* Normalization statistics

**Dynamic behavior.** Precision is increasingly dynamic at runtime. **Likely Scenario:** KV caches often escalate from INT8/FP8 to FP16 as context length grows to prevent numerical drift.

**Interpretation.** Fallbacks are not inefficiencies; they are *structural safeguards*.

---

## 5. Attention and LayerNorm

**Observation.** Attention and normalization are the least precision-tolerant components in the entire stack.

### Attention

Attention combines dot products, exponentiation, and normalization. Small numerical errors in any step can radically change token weighting.

* QK dot products usually remain in **BF16/FP16**.
* Softmax logits are almost always computed in higher precision to prevent overflow.
* KV caches receive special handling for long sequences (often involving dynamic rescaling).

### LayerNorm / RMSNorm

Normalization layers compute global statistics. Quantization here affects *every downstream operation*.

* Mean/variance (or RMS) estimation is typically performed in **BF16 or FP32**.
* **Disclaimer:** Aggressive quantization in normalization leads to variance collapse and training divergence.

---

## 6. Case Studies: The 2025/2026 Cohort Estimated Precision Profiles

By early 2026, the industry has moved beyond monolithic precision. The major labs employ distinct strategies based on their hardware lineage (GPU vs. TPU).

### Gemini 3.0 Pro (Google)

* **Architecture:** Sparse Mixture-of-Experts (MoE).
* **Precision Strategy (Estimated):** **INT8 / BF16 Hybrid.**
* As a TPU-native model (TPU v5/v6/v7), Gemini utilizes the TPU's robust BF16 paths.
* For serving, expert weights are **likely INT8 quantized** to maximize memory bandwidth, while active calculation paths remain in BF16.


* **The "Thinking Level":** Gemini 3 introduced "Deep Think" modes. This effectively trades latency for *computational precision* via hidden chain-of-thought tokens.

### GPT-5.2 (OpenAI)

* **Variants:** `instant` (latency-optimized) and `thinking` (reasoning-optimized).
* **Precision Strategy:** **FP8 (Float8) Native.**
* Leveraging the **NVIDIA Blackwell** infrastructure, GPT-5.2 utilizes native FP8 optimization via the Transformer Engine.
* **GPT-5.2 Instant:** Likely utilizes aggressive **INT4 or NF4** quantization for weights to maximize token throughput for everyday tasks.



### Claude Sonnet 4.5 (Anthropic)

* **Focus:** Reliability, Coding, and Agentic Workflows.
* **Precision Strategy:** **FP8 Conservative.**
* Anthropic typically avoids ultra-aggressive quantization that risks "faithfulness."
* FP8 serves as the "Goldilocks" zone: halving memory usage vs BF16 while preserving the subtle reasoning required for complex coding.



| Feature | Gemini 3.0 Pro | GPT-5.2 | Sonnet 4.5 |
| --- | --- | --- | --- |
| **Inference Weights** | INT8 / BF16 (Hybrid) | FP8 / INT4 (Instant) | FP8 (Float8) |
| **Inference Activations** | BF16 | FP8 | FP8 / BF16 |
| **Hardware Bias** | TPU (Google) | GPU (NVIDIA) | GPU (NVIDIA) |

---

## 7. The Hardware Substrate (Early 2026)

### The Contenders

1. **Ironwood (Google TPU v7):**
* **Status:** GA as of November 2025.
* **Scaling:** Connects **9,216 chips** into a single pod via Optical Circuit Switches.
* **Precision:** Heavily optimized for BF16 and INT8, delivering ~4.6 PFLOPS per chip.


2. **Blackwell Ultra (NVIDIA B300):**
* **Philosophy:** Memory Density.
* **Key Stat:** **288GB HBM3e** per chip. This allows massive batch sizes on a single device.
* **Precision:** The definitive **FP8 engine** (scaling to FP4 in specialized modes).



### The Groq Factor

In a major late-2025 shift, **NVIDIA finalized a $20B deal** to license Groq’s LPU technology and bring their core talent in-house. This signals a strategic move toward specialized, deterministic inference units to solve the "latency floor" issue in general-purpose GPUs.

---

## Closing Synthesis

Across frontier models, a consistent ordering regarding precision tolerance remains valid:

![Precision Sensitivity](/images/precision-sensitivity.png)

Modern "mixed precision" now encompasses three dimensions:

1. **Numerical:** (FP4/FP8/BF16) tailored to the specific layer.
2. **Temporal:** (Thinking steps) extra tokens allocated to hard problems.
3. **Architectural:** (General GPU vs. Dedicated LPU) tailored to the workload phase.

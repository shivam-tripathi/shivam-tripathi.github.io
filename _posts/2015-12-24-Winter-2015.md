---
layout: post
title: Neural Networks - Sigmoid Neurons
---

![Neural Nets]({{ site.baseurl }}/images/nn/one.png "Neural Networks")

Perceptrons were developed in 1950s and 1960s by Frank Rosenblatt, inspired by earlier works of Warren McCulloch and Walter Pitts. Perceptrons serve as the most basic types of neurons used in learning.

Perceptrons can be used for universal computation, there is no logical circuit which cannot be realized using perceptron. This is essentially true as universal gates NAND and NOR can be easily implemented using perceptron. Single layer perceptrons can be used to implement basic gates like AND, NOT, OR, NAND etc. But gates like XOR need multi-layer perceptrons.

The computational universality of perceptrons is reassuring, as learning algorithms can be used to tune the weights based on inputs and difference between desired output and output given by the perceptrons.

**Sigmoid Neurons**
To have controlled learning, our main objective is to have a small change in weights lead to a small change in output of the network. This isn't what happens when our neuron is a perceptron. This is so as output is completely flipped (the output is either 0 or 1) with a certain set of weights. When we are aiming for multiple outputs for a certain certain of inputs - this may lead to setting of weights which may be very accurate for a particular result, but might affect other results in an uncertain manner.
To overcome this problem, sigmoid neurons are used. They are similar to perceptrons, but are modified so as to give only a small change in output for a small change in the inputs.
Inputs to a sigmoid neuron can be any real number between 0 and 1, and also it has weights and overall bias. But the output is not 0 or 1, it is \sigma(w.x + b) where \sigma is sigmoid function.
$$\sigma(z) = \frac{1}{1 + e^(-z)}$$
For very large positive and negative values of (w.x + b), the outputs match those of perceptrons. But when (w.x + b) assumes modest values, there is a major change.
If we compare perceptron with a sigmoid neuron graphically, the perceptron would act like a step function, while sigmoid will act like a smoothed out perceptron. This smoothness will result in a small change in output of the network for a small change in weights.

